# -*- coding: utf-8 -*-
"""vptq_hf_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/microsoft/VPTQ/blob/main/notebooks/vptq_hf_example.ipynb
"""

! pip install git+https://github.com/huggingface/transformers.git -U
! pip install vptq -U

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load VPTQ-quantized model directly from HuggingFace Hub
model = AutoModelForCausalLM.from_pretrained("VPTQ-community/Qwen2.5-32B-Instruct-v8-k256-256-woft", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("VPTQ-community/Qwen2.5-32B-Instruct-v8-k256-256-woft")

# Simple inference
prompt = "Explain: Do not go gentle into that good night."
output = model.generate(**tokenizer(prompt, return_tensors="pt").to(model.device))
print(tokenizer.decode(output[0], skip_special_tokens=True))

! pip install git+https://github.com/huggingface/transformers.git -U
! pip install vptq -U

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load VPTQ-quantized model directly from HuggingFace Hub
model = AutoModelForCausalLM.from_pretrained("VPTQ-community/Qwen2.5-32B-Instruct-v8-k256-256-woft", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("VPTQ-community/Qwen2.5-32B-Instruct-v8-k256-256-woft")

# Simple inference
prompt = "Explain: Do not go gentle into that good night."
output = model.generate(**tokenizer(prompt, return_tensors="pt").to(model.device))
print(tokenizer.decode(output[0], skip_special_tokens=True))





prompt = "Who is Charlie Chaplin?"
output = model.generate(**tokenizer(prompt, return_tensors="pt").to(model.device))
print(tokenizer.decode(output[0], skip_special_tokens=True))

prompt = "Who is Charlie Chaplin?"
output = model.generate(**tokenizer(prompt, return_tensors="pt").to(model.device), do_sample=True, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))

prompt = "write python code for print number 3 six times"
output = model.generate(**tokenizer(prompt, return_tensors="pt").to(model.device), do_sample=True, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))

for i in range(6):
    print("3")

prompt = "Write 3 sentences ending with the word beauty."
output = model.generate(**tokenizer(prompt, return_tensors="pt").to(model.device), do_sample=True, max_new_tokens=128)
print(tokenizer.decode(output[0], skip_special_tokens=True))